{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Introduction with Direct OpenAI and Pinecone APIs\n",
        "\n",
        "This notebook demonstrates the complete RAG pipeline using **direct API calls** to OpenAI and Pinecone (without LangChain): document loading, chunking, embeddings, vector database setup, and retrieval-augmented generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install required packages\n",
        "!pip install openai pinecone pypdf python-dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Verify API keys are loaded\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in environment variables. Please create a .env file with your API key.\")\n",
        "if not os.getenv(\"CONE_APPINEI_KEY\"):\n",
        "    raise ValueError(\"PINECONE_API_KEY not found in environment variables. Please create a .env file with your API key.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Document Loading and Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For demonstration, create sample documents\n",
        "# In production, you would load from PDF using pypdf:\n",
        "# from pypdf import PdfReader\n",
        "# reader = PdfReader(\"path/to/your/document.pdf\")\n",
        "# text = \"\".join([page.extract_text() for page in reader.pages])\n",
        "\n",
        "documents = [\n",
        "    {\"page_content\": \"Machine learning is a subset of artificial intelligence that enables systems to learn from data.\"},\n",
        "    {\"page_content\": \"Deep learning uses neural networks with multiple layers to process complex patterns.\"},\n",
        "    {\"page_content\": \"Natural language processing allows computers to understand and generate human language.\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 3\n",
            "First chunk: Machine learning is a subset of artificial intelligence that enables systems to learn from data....\n"
          ]
        }
      ],
      "source": [
        "# Custom chunking function (replaces RecursiveCharacterTextSplitter)\n",
        "def chunk_text(text, chunk_size=500, chunk_overlap=50):\n",
        "    \"\"\"\n",
        "    Split text into chunks with overlap.\n",
        "    \n",
        "    Args:\n",
        "        text: The text to chunk\n",
        "        chunk_size: Maximum size of each chunk\n",
        "        chunk_overlap: Number of characters to overlap between chunks\n",
        "    \n",
        "    Returns:\n",
        "        List of chunk dictionaries with 'page_content' and 'metadata'\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    \n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        \n",
        "        chunks.append({\n",
        "            \"page_content\": chunk,\n",
        "            \"metadata\": {\"start\": start, \"end\": end}\n",
        "        })\n",
        "        \n",
        "        # Move start position forward, accounting for overlap\n",
        "        start += chunk_size - chunk_overlap\n",
        "        \n",
        "        # Prevent infinite loop if chunk_size <= chunk_overlap\n",
        "        if chunk_size <= chunk_overlap:\n",
        "            break\n",
        "    \n",
        "    return chunks\n",
        "\n",
        "def chunk_documents(documents, chunk_size=500, chunk_overlap=50):\n",
        "    \"\"\"\n",
        "    Chunk a list of documents.\n",
        "    \n",
        "    Args:\n",
        "        documents: List of document dictionaries with 'page_content'\n",
        "        chunk_size: Maximum size of each chunk\n",
        "        chunk_overlap: Number of characters to overlap between chunks\n",
        "    \n",
        "    Returns:\n",
        "        List of chunk dictionaries\n",
        "    \"\"\"\n",
        "    all_chunks = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        text = doc[\"page_content\"]\n",
        "        chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
        "        # Add document index to metadata\n",
        "        for chunk in chunks:\n",
        "            chunk[\"metadata\"][\"document_index\"] = i\n",
        "        all_chunks.extend(chunks)\n",
        "    return all_chunks\n",
        "\n",
        "# Chunk the documents\n",
        "chunks = chunk_documents(documents, chunk_size=500, chunk_overlap=50)\n",
        "print(f\"Number of chunks: {len(chunks)}\")\n",
        "print(f\"First chunk: {chunks[0]['page_content'][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Chunking matters when documents are large or structured. For simple use cases with small documents, you might skip chunking entirely. Recognize when chunking adds value vs when it's unnecessary complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding dimension: 1536\n",
            "First 5 values: [-0.01686064898967743, -0.0005186386406421661, 0.020080123096704483, -0.015040520578622818, 0.07785451412200928]\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "# Embedding functions (replaces OpenAIEmbeddings)\n",
        "def embed_query(text, client, model=\"text-embedding-3-small\"):\n",
        "    \"\"\"\n",
        "    Create an embedding for a single query text.\n",
        "    \n",
        "    Args:\n",
        "        text: The text to embed\n",
        "        client: OpenAI client instance\n",
        "        model: Embedding model to use\n",
        "    \n",
        "    Returns:\n",
        "        List of embedding values\n",
        "    \"\"\"\n",
        "    response = client.embeddings.create(\n",
        "        model=model,\n",
        "        input=text\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "def embed_documents(texts, client, model=\"text-embedding-3-small\"):\n",
        "    \"\"\"\n",
        "    Create embeddings for multiple texts (batch processing).\n",
        "    \n",
        "    Args:\n",
        "        texts: List of texts to embed\n",
        "        client: OpenAI client instance\n",
        "        model: Embedding model to use\n",
        "    \n",
        "    Returns:\n",
        "        List of embedding vectors\n",
        "    \"\"\"\n",
        "    response = client.embeddings.create(\n",
        "        model=model,\n",
        "        input=texts\n",
        "    )\n",
        "    return [item.embedding for item in response.data]\n",
        "\n",
        "# Create embeddings for a sample text\n",
        "sample_text = \"Machine learning enables systems to learn from data\"\n",
        "sample_embedding = embed_query(sample_text, openai_client)\n",
        "print(f\"Embedding dimension: {len(sample_embedding)}\")\n",
        "print(f\"First 5 values: {sample_embedding[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Embedding costs add up with large document collections. Consider cheaper embedding models for MVPs, and upgrade only when quality matters. Understand the cost implications before committing to a solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Pinecone Vector Store Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'PINECONE_API_KEY'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpinecone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pinecone, ServerlessSpec\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialize Pinecone client\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m pc = Pinecone(api_key=\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPINECONE_API_KEY\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Create or connect to an index\u001b[39;00m\n\u001b[32m      7\u001b[39m index_name = \u001b[33m\"\u001b[39m\u001b[33mrag-openai-index\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:714\u001b[39m, in \u001b[36m__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n",
            "\u001b[31mKeyError\u001b[39m: 'PINECONE_API_KEY'"
          ]
        }
      ],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
        "\n",
        "# Create or connect to an index\n",
        "index_name = \"rag-openai-index\"\n",
        "\n",
        "# Check if index exists, create if not\n",
        "existing_indexes = [index.name for index in pc.list_indexes()]\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,  # OpenAI text-embedding-3-small dimension\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "    print(f\"Created index: {index_name}\")\n",
        "else:\n",
        "    print(f\"Index {index_name} already exists\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 3 documents to Pinecone vector store\n"
          ]
        }
      ],
      "source": [
        "# Connect to the index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "# Create embeddings for all chunks\n",
        "chunk_texts = [chunk[\"page_content\"] for chunk in chunks]\n",
        "chunk_embeddings = embed_documents(chunk_texts, openai_client)\n",
        "\n",
        "# Prepare vectors for upsert (replaces PineconeVectorStore.from_documents)\n",
        "vectors_to_upsert = []\n",
        "for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
        "    vectors_to_upsert.append({\n",
        "        \"id\": f\"chunk-{i}\",\n",
        "        \"values\": embedding,\n",
        "        \"metadata\": {\n",
        "            \"text\": chunk[\"page_content\"],\n",
        "            **chunk[\"metadata\"]\n",
        "        }\n",
        "    })\n",
        "\n",
        "# Upsert vectors to Pinecone\n",
        "index.upsert(vectors=vectors_to_upsert)\n",
        "print(f\"Added {len(vectors_to_upsert)} documents to Pinecone vector store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Pinecone is powerful but adds infrastructure complexity and cost. For small projects or MVPs, consider simpler alternatives like in-memory vector stores or Chroma. Use Pinecone when you need scale, performance, or managed infrastructure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Query and Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: What is machine learning?\n",
            "\n",
            "Retrieved 2 documents:\n",
            "\n",
            "1. Machine learning is a subset of artificial intelligence that enables systems to learn from data.\n",
            "\n",
            "2. Deep learning uses neural networks with multiple layers to process complex patterns.\n"
          ]
        }
      ],
      "source": [
        "# Perform a similarity search (replaces vectorstore.similarity_search)\n",
        "def similarity_search(query, index, openai_client, k=2):\n",
        "    \"\"\"\n",
        "    Search for similar documents using query embedding.\n",
        "    \n",
        "    Args:\n",
        "        query: Query text\n",
        "        index: Pinecone index instance\n",
        "        openai_client: OpenAI client instance\n",
        "        k: Number of results to return\n",
        "    \n",
        "    Returns:\n",
        "        List of document dictionaries with 'page_content' and 'metadata'\n",
        "    \"\"\"\n",
        "    # Embed the query\n",
        "    query_embedding = embed_query(query, openai_client)\n",
        "    \n",
        "    # Query Pinecone\n",
        "    results = index.query(\n",
        "        vector=query_embedding,\n",
        "        top_k=k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    \n",
        "    # Format results\n",
        "    documents = []\n",
        "    for match in results.matches:\n",
        "        documents.append({\n",
        "            \"page_content\": match.metadata[\"text\"],\n",
        "            \"metadata\": {k: v for k, v in match.metadata.items() if k != \"text\"}\n",
        "        })\n",
        "    \n",
        "    return documents\n",
        "\n",
        "# Perform a search\n",
        "query = \"What is machine learning?\"\n",
        "results = similarity_search(query, index, openai_client, k=2)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"\\nRetrieved {len(results)} documents:\")\n",
        "for i, doc in enumerate(results, 1):\n",
        "    print(f\"\\n{i}. {doc['page_content']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'query' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m documents_with_scores\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Get results with scores\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m results_with_scores = similarity_search_with_score(\u001b[43mquery\u001b[49m, index, openai_client, k=\u001b[32m2\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mRetrieved documents with scores:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'query' is not defined"
          ]
        }
      ],
      "source": [
        "# Get similarity scores (replaces vectorstore.similarity_search_with_score)\n",
        "def similarity_search_with_score(query, index, openai_client, k=2):\n",
        "    \"\"\"\n",
        "    Search for similar documents with similarity scores.\n",
        "    \n",
        "    Args:\n",
        "        query: Query text\n",
        "        index: Pinecone index instance\n",
        "        openai_client: OpenAI client instance\n",
        "        k: Number of results to return\n",
        "    \n",
        "    Returns:\n",
        "        List of tuples (document_dict, score)\n",
        "    \"\"\"\n",
        "    # Embed the query\n",
        "    query_embedding = embed_query(query, openai_client)\n",
        "    \n",
        "    # Query Pinecone\n",
        "    results = index.query(\n",
        "        vector=query_embedding,\n",
        "        top_k=k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    \n",
        "    # Format results with scores\n",
        "    documents_with_scores = []\n",
        "    for match in results.matches:\n",
        "        doc = {\n",
        "            \"page_content\": match.metadata[\"text\"],\n",
        "            \"metadata\": {k: v for k, v in match.metadata.items() if k != \"text\"}\n",
        "        }\n",
        "        documents_with_scores.append((doc, match.score))\n",
        "    \n",
        "    return documents_with_scores\n",
        "\n",
        "# Get results with scores\n",
        "results_with_scores = similarity_search_with_score(query, index, openai_client, k=2)\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "print(f\"\\nRetrieved documents with scores:\")\n",
        "for doc, score in results_with_scores:\n",
        "    print(f\"\\nScore: {score:.4f}\")\n",
        "    print(f\"Content: {doc['page_content']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Retrieval quality varies with chunking strategy and embedding model. Test retrieval before building the full RAG system. If retrieval consistently fails, the problem might be with chunking or embeddings, not the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Complete RAG Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete RAG implementation using direct API calls (replaces LCEL chain)\n",
        "\n",
        "# Initialize OpenAI chat client\n",
        "chat_client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "def format_docs(docs):\n",
        "    \"\"\"Format retrieved documents into a single string.\"\"\"\n",
        "    return \"\\n\\n\".join(doc[\"page_content\"] for doc in docs)\n",
        "\n",
        "def rag_query(question, index, openai_client, chat_client, k=2):\n",
        "    \"\"\"\n",
        "    Complete RAG pipeline: query embedding → Pinecone search → OpenAI chat.\n",
        "    \n",
        "    Args:\n",
        "        question: User's question\n",
        "        index: Pinecone index instance\n",
        "        openai_client: OpenAI client for embeddings\n",
        "        chat_client: OpenAI client for chat completions\n",
        "        k: Number of documents to retrieve\n",
        "    \n",
        "    Returns:\n",
        "        Answer string from the LLM\n",
        "    \"\"\"\n",
        "    # Step 1: Embed the query\n",
        "    query_embedding = embed_query(question, openai_client)\n",
        "    \n",
        "    # Step 2: Search Pinecone for relevant documents\n",
        "    search_results = index.query(\n",
        "        vector=query_embedding,\n",
        "        top_k=k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    \n",
        "    # Step 3: Format retrieved context\n",
        "    context_docs = []\n",
        "    for match in search_results.matches:\n",
        "        context_docs.append({\n",
        "            \"page_content\": match.metadata[\"text\"]\n",
        "        })\n",
        "    context = format_docs(context_docs)\n",
        "    \n",
        "    # Step 4: Create RAG prompt\n",
        "    rag_prompt = f\"\"\"Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    \n",
        "    # Step 5: Call OpenAI chat completions\n",
        "    response = chat_client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": rag_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "    \n",
        "    # Step 6: Extract and return the answer\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Ask a question\n",
        "question = \"What is machine learning?\"\n",
        "response = rag_query(question, index, openai_client, chat_client, k=2)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"\\nAnswer: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Direct API calls make each step of the RAG pipeline explicit and transparent — embedding → retrieval → formatting → prompting → LLM → parsing. This approach gives you full control but requires more code than using frameworks like LangChain. Use direct APIs when you need fine-grained control, want to understand the underlying mechanics, or prefer minimal dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Comparison: With vs Without RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Without RAG: Direct API call\n",
        "simple_response = chat_client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n",
        "    ],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(\"Without RAG (direct API call):\")\n",
        "print(simple_response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# With RAG: Context from vector database\n",
        "rag_response = rag_query(\"What is machine learning?\", index, openai_client, chat_client, k=2)\n",
        "print(\"With RAG (retrieved context):\")\n",
        "print(rag_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Scoping Insight**: Compare the complexity and cost of both approaches. RAG is powerful but requires infrastructure, embeddings, and retrieval logic. Simple API calls work for many use cases. Recognize when the added complexity of RAG is justified by the requirements."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ironhack_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
