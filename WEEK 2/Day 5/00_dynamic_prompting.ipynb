{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dynamic Prompting\n",
        "\n",
        "Dynamic prompting means constructing prompts **at runtime** by inserting variables, data, or user input — instead of hardcoding every word. This is what makes AI applications flexible and data-driven: the same code can handle thousands of different inputs.\n",
        "\n",
        "**Why it matters:** Every real-world AI application — from chatbots to RAG systems — relies on dynamic prompts. Without them, you'd need a separate hardcoded prompt for every possible question.\n",
        "\n",
        "**Today's journey:** Basic dynamic prompting → conversation memory → context injection patterns (which leads directly into RAG)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!pip install openai python-dotenv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\"OPENAI_API_KEY not found. Please create a .env file with your API key.\")\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Static vs. Dynamic Prompts\n",
        "\n",
        "A **static prompt** is fixed in your code. It always sends the exact same text to the model, no matter what the user wants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Static prompt — hardcoded, inflexible\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"What is photosynthesis?\"}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This works, but it will **always** ask about photosynthesis. If you want to ask about something else, you have to change the code.\n",
        "\n",
        "A **dynamic prompt** inserts variables at runtime, so the same code handles any question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dynamic prompt — adapts to any input\n",
        "user_question = \"What is machine learning?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": f\"Answer this question concisely: {user_question}\"}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Same code, different question — no code change needed\n",
        "user_question = \"What is the capital of France?\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[{\"role\": \"user\", \"content\": f\"Answer this question concisely: {user_question}\"}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The API doesn't know or care whether the prompt was hardcoded or built dynamically — it just receives a string. That's the key insight: **prompts are just strings**, and we can construct them however we want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Simple Dynamic Prompts\n",
        "\n",
        "The core idea: wrap your prompt logic in a **function** that takes parameters and returns a response. This separates the data layer (variables) from the prompt layer (template)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask(question):\n",
        "    \"\"\"Send a dynamic question to the model.\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Answer this question concisely: {question}\"}],\n",
        "        temperature=0\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Now any question works with the same function\n",
        "print(ask(\"What is an API?\"))\n",
        "print(\"---\")\n",
        "print(ask(\"Explain recursion in one sentence.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding More Parameters\n",
        "\n",
        "Dynamic prompts can inject more than just the question — you can control tone, format, length, or any other behavior at runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_with_style(question, tone=\"professional\", max_sentences=2):\n",
        "    \"\"\"Send a question with configurable tone and length.\"\"\"\n",
        "    prompt = (\n",
        "        f\"Answer the following question in a {tone} tone. \"\n",
        "        f\"Keep your response to {max_sentences} sentences maximum.\\n\\n\"\n",
        "        f\"Question: {question}\"\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Same question, different styles\n",
        "print(\"Professional:\")\n",
        "print(ask_with_style(\"What is cloud computing?\", tone=\"professional\"))\n",
        "print(\"\\nCasual:\")\n",
        "print(ask_with_style(\"What is cloud computing?\", tone=\"casual and friendly\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Injecting Context\n",
        "\n",
        "The most powerful use of dynamic prompting: injecting **external data** into the prompt so the model can answer based on information it wasn't trained on. This is the foundation of RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer_with_context(question, context):\n",
        "    \"\"\"Answer a question using provided context.\"\"\"\n",
        "    prompt = (\n",
        "        f\"Use the following context to answer the question. \"\n",
        "        f\"If the context doesn't contain the answer, say so.\\n\\n\"\n",
        "        f\"Context:\\n{context}\\n\\n\"\n",
        "        f\"Question: {question}\"\n",
        "    )\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Company-specific info the model was never trained on\n",
        "company_context = \"\"\"\n",
        "Acme Corp was founded in 2019. The CEO is Jane Smith.\n",
        "Acme Corp has 150 employees and is headquartered in Berlin.\n",
        "Their main product is an AI-powered inventory management system.\n",
        "\"\"\"\n",
        "\n",
        "print(answer_with_context(\"Who is the CEO of Acme Corp?\", company_context))\n",
        "print(\"---\")\n",
        "print(answer_with_context(\"What does Acme Corp sell?\", company_context))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the pattern: we build a prompt string that combines a **template** (the instructions), **context** (external data), and a **question** (user input). This is exactly what a RAG system does — except RAG retrieves the context automatically from a vector database instead of hardcoding it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conversation Memory\n",
        "\n",
        "Another form of dynamic prompting: building up the `messages` list at runtime to give the model memory of the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_with_memory():\n",
        "    \"\"\"Simple chat loop that maintains conversation history.\"\"\"\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant. Be concise.\"}]\n",
        "\n",
        "    questions = [\n",
        "        \"What is Python?\",\n",
        "        \"What are its main use cases?\",  # 'its' refers to Python — the model needs memory\n",
        "        \"Which one is best for beginners?\"\n",
        "    ]\n",
        "\n",
        "    for question in questions:\n",
        "        messages.append({\"role\": \"user\", \"content\": question})\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=messages,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "        answer = response.choices[0].message.content\n",
        "        messages.append({\"role\": \"assistant\", \"content\": answer})\n",
        "\n",
        "        print(f\"User: {question}\")\n",
        "        print(f\"Assistant: {answer}\\n\")\n",
        "\n",
        "chat_with_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each turn, the `messages` list grows dynamically. The model sees the full conversation history and can resolve references like \"its\" and \"which one.\" This is dynamic prompting in action — the prompt is constructed at runtime from accumulated data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "- **Prompts are just strings** — build them with f-strings, functions, or any Python logic.\n",
        "- **Separate data from template** — your function takes parameters; your prompt template stays fixed.\n",
        "- **Context injection** is the bridge to RAG: today we hardcode the context, but RAG retrieves it from a database.\n",
        "- **Conversation memory** is another dynamic prompting pattern: the messages list grows at runtime.\n",
        "\n",
        "**Next up:** In the RAG lesson, we'll replace the hardcoded context with automatic retrieval from a vector database — so the right information is always injected into the prompt."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "consulting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}