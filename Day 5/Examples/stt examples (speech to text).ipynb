{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e837c792",
      "metadata": {},
      "source": [
        "# What does your voice look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e5660b0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "duration = 5  # seconds\n",
        "sample_rate = 16000\n",
        "\n",
        "print(f\"üé§ Recording for {duration} seconds... Speak now!\")\n",
        "audio = sd.rec(int(duration * sample_rate), \n",
        "               samplerate=sample_rate, \n",
        "               channels=1, \n",
        "               dtype='float32')\n",
        "sd.wait()\n",
        "audio = audio.flatten()\n",
        "print(\"‚úÖ Recording complete!\")\n",
        "\n",
        "\n",
        "print(\"This is your voice\")\n",
        "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
        "\n",
        "time = np.linspace(0, len(audio) / sample_rate, len(audio))\n",
        "axes[0].plot(time, audio, color='blue', linewidth=0.5)\n",
        "axes[0].set_title('Your Voice Waveform', fontsize=14)\n",
        "axes[0].set_xlabel('Time (seconds)')\n",
        "axes[0].set_ylabel('Amplitude')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "print(\"Mel Spectogram\") \n",
        "mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=80)\n",
        "mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "img = librosa.display.specshow(mel_spec_db, sr=sample_rate, \n",
        "                                x_axis='time', y_axis='mel', \n",
        "                                ax=axes[1], cmap='viridis')\n",
        "axes[1].set_title('Mel Spectrogram - Your Voice as a Picture!', fontsize=14)\n",
        "plt.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
        "\n",
        "print(\"Spectogram\") \n",
        "D = librosa.stft(audio)\n",
        "D_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "img2 = librosa.display.specshow(D_db, sr=sample_rate, \n",
        "                                 x_axis='time', y_axis='hz', \n",
        "                                 ax=axes[2], cmap='magma')\n",
        "axes[2].set_title('Frequency Spectrogram', fontsize=14)\n",
        "axes[2].set_ylim(0, 4000)  # Focus on speech frequencies\n",
        "plt.colorbar(img2, ax=axes[2], format='%+2.0f dB')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d4f14a",
      "metadata": {},
      "source": [
        "# Whisper API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96047ace",
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.io.wavfile as wavfile\n",
        "from openai import OpenAI\n",
        "import io\n",
        "import os\n",
        "from IPython.display import Audio, display\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "duration = 5  # seconds\n",
        "sample_rate = 16000\n",
        "\n",
        "print(f\"üé§ Recording for {duration} seconds... Speak now!\")\n",
        "audio = sd.rec(int(duration * sample_rate), \n",
        "               samplerate=sample_rate, \n",
        "               channels=1, \n",
        "               dtype='float32')\n",
        "sd.wait()\n",
        "audio = audio.flatten()\n",
        "print(\"‚úÖ Recording complete!\")\n",
        "\n",
        "\n",
        "audio_int16 = np.int16(audio * 32767)\n",
        "\n",
        "# Create in-memory WAV file\n",
        "buffer = io.BytesIO()\n",
        "wavfile.write(buffer, sample_rate, audio_int16)\n",
        "buffer.seek(0)\n",
        "buffer.name = \"recording.wav\"  # Whisper needs a filename\n",
        "\n",
        "# Transcribe with Whisper\n",
        "print(\"ü§ñ Transcribing with Whisper...\")\n",
        "transcript = client.audio.transcriptions.create(\n",
        "    model=\"whisper-1\",\n",
        "    file=buffer\n",
        ")\n",
        "\n",
        "print(\"\\nüìù Transcription:\")\n",
        "print(\"-\" * 40)\n",
        "print(transcript.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7066ebf",
      "metadata": {},
      "source": [
        "# Super Power: Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb61ab84",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wavfile\n",
        "from openai import OpenAI\n",
        "import io\n",
        "from IPython.display import Audio, display, Markdown\n",
        "\n",
        "#client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Display a 30-second reading text\n",
        "reading_text = \"\"\"\n",
        "**Read this text aloud (about 30 seconds):**\n",
        "\n",
        "The researchers' analyses of the wind farm data showed variable wind patterns. The project \n",
        "lead will lead the team through complex permit processes. They must present the present \n",
        "findings to investors. The 3 million contract includes clauses for force majeure \n",
        "events. Dr. Garc√≠a-Smith's team discovered that the enzyme's pH of 7.2 was optimal. The \n",
        "AI learned to differentiate between \"read\" (present) and \"read\" (past), processing live \n",
        "feeds versus archived content. The baroque bass player stood close to the entrance, too \n",
        "close to record properly.\n",
        "\"\"\"\n",
        "\n",
        "display(Markdown(reading_text))\n",
        "\n",
        "# Record for 30 seconds\n",
        "duration = 45\n",
        "sample_rate = 16000\n",
        "\n",
        "print(f\"\\nüé§ Recording for {duration} seconds... Start reading now!\")\n",
        "audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')\n",
        "sd.wait()\n",
        "audio = audio.flatten()\n",
        "print(\"‚úÖ Recording complete!\")\n",
        "\n",
        "display(Audio(audio, rate=sample_rate))\n",
        "\n",
        "\n",
        "# Prepare audio\n",
        "audio_int16 = np.int16(audio * 32767)\n",
        "buffer = io.BytesIO()\n",
        "wavfile.write(buffer, sample_rate, audio_int16)\n",
        "buffer.seek(0)\n",
        "buffer.name = \"recording.wav\"\n",
        "\n",
        "# Transcribe without context prompt\n",
        "print(\"ü§ñ Transcribing without context...\")\n",
        "transcript = client.audio.transcriptions.create(\n",
        "    model=\"whisper-1\",\n",
        "    file=buffer,\n",
        ")\n",
        "\n",
        "print(\"\\nüìù Transcription:\")\n",
        "print(\"-\" * 40)\n",
        "print(transcript.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a582f990",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Transcribe with context prompt\n",
        "print(\"ü§ñ Transcribing with context...\")\n",
        "transcript = client.audio.transcriptions.create(\n",
        "    model=\"whisper-1\",\n",
        "    file=buffer,\n",
        "    prompt = \"heteronyms, homographs, lead metal, wind turbine, present tense, past tense read, pH levels, enzyme, Garc√≠a-Smith, force majeure, baroque, bass instrument, differentiate\")\n",
        "\n",
        "print(\"\\nüìù Transcription:\")\n",
        "print(\"-\" * 40)\n",
        "print(transcript.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebe465bd",
      "metadata": {},
      "source": [
        "\n",
        "# Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49731fa2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wavfile\n",
        "from openai import OpenAI\n",
        "import io\n",
        "\n",
        "#client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Record longer audio\n",
        "duration = 15  # seconds total\n",
        "sample_rate = 16000\n",
        "chunk_duration = 5  # seconds per chunk\n",
        "\n",
        "print(f\"üé§ Recording for {duration} seconds... Speak continuously!\")\n",
        "audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')\n",
        "sd.wait()\n",
        "audio = audio.flatten()\n",
        "print(\"‚úÖ Recording complete!\")\n",
        "\n",
        "display(Audio(audio, rate=sample_rate))\n",
        "\n",
        "\n",
        "# Split audio into chunks\n",
        "chunk_size = chunk_duration * sample_rate\n",
        "chunks = [audio[i:i+chunk_size] for i in range(0, len(audio), chunk_size)]\n",
        "\n",
        "print(f\"\\nüî™ Split into {len(chunks)} chunks\")\n",
        "\n",
        "# Transcribe each chunk\n",
        "all_transcripts = []\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"\\nü§ñ Transcribing chunk {i+1}/{len(chunks)}...\")\n",
        "    \n",
        "    # Prepare chunk buffer\n",
        "    chunk_int16 = np.int16(chunk * 32767)\n",
        "    buffer = io.BytesIO()\n",
        "    wavfile.write(buffer, sample_rate, chunk_int16)\n",
        "    buffer.seek(0)\n",
        "    buffer.name = f\"chunk_{i}.wav\"\n",
        "    \n",
        "    # Transcribe\n",
        "    transcript = client.audio.transcriptions.create(\n",
        "        model=\"whisper-1\",\n",
        "        file=buffer\n",
        "    )\n",
        "    \n",
        "    all_transcripts.append(transcript.text)\n",
        "    print(f\"Chunk {i+1}: {transcript.text}\")\n",
        "\n",
        "# Combine all transcripts\n",
        "print(\"\\nüìù Complete Transcription:\")\n",
        "print(\"-\" * 40)\n",
        "full_text = \" \".join(all_transcripts)\n",
        "print(full_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29767d1",
      "metadata": {},
      "source": [
        "# Time Stamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "853506d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sounddevice as sd\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wavfile\n",
        "from openai import OpenAI\n",
        "import io\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "#client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "duration = 10  # seconds\n",
        "sample_rate = 16000\n",
        "\n",
        "print(f\"üé§ Recording for {duration} seconds... Speak with pauses!\")\n",
        "audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')\n",
        "sd.wait()\n",
        "audio = audio.flatten()\n",
        "print(\"‚úÖ Recording complete!\")\n",
        "\n",
        "# Play back\n",
        "display(Audio(audio, rate=sample_rate))\n",
        "\n",
        "# Prepare audio\n",
        "audio_int16 = np.int16(audio * 32767)\n",
        "buffer = io.BytesIO()\n",
        "wavfile.write(buffer, sample_rate, audio_int16)\n",
        "buffer.seek(0)\n",
        "buffer.name = \"recording.wav\"\n",
        "\n",
        "# Get detailed transcription with timestamps\n",
        "print(\"\\nü§ñ Transcribing with timestamps...\")\n",
        "transcript = client.audio.transcriptions.create(\n",
        "    model=\"whisper-1\",\n",
        "    file=buffer,\n",
        "    response_format=\"verbose_json\",\n",
        "    timestamp_granularities=[\"segment\", \"word\"]  # Get both segment and word timestamps\n",
        ")\n",
        "\n",
        "# Display formatted timestamps\n",
        "print(\"\\nüìù Transcription with timestamps:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Show segments with timestamps\n",
        "if hasattr(transcript, 'segments'):\n",
        "    for segment in transcript.segments:\n",
        "        start = segment.start\n",
        "        end = segment.end\n",
        "        text = segment.text\n",
        "        print(f\"\\n[{start:.2f}s - {end:.2f}s]\")\n",
        "        print(f\"  {text}\")\n",
        "\n",
        "# Show word-level timestamps if available\n",
        "if hasattr(transcript, 'words'):\n",
        "    print(\"\\nüî§ Word-level timing:\")\n",
        "    print(\"-\" * 40)\n",
        "    for word in transcript.words:\n",
        "        word_text = word.word\n",
        "        start = word.start\n",
        "        end = word.end\n",
        "        print(f\"{word_text:15} [{start:.2f}s - {end:.2f}s]\")\n",
        "\n",
        "# Full text and metadata\n",
        "print(\"\\nüìÑ Full text:\")\n",
        "print(transcript.text)\n",
        "\n",
        "print(f\"\\nüåç Language detected: {transcript.language}\")\n",
        "print(f\"‚è±Ô∏è Total duration: {transcript.duration:.2f}s\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "consulting",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}